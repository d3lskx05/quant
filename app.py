# app.py
import os
import zipfile
import time
import traceback
from pathlib import Path
from functools import lru_cache
from typing import Optional, List

import gdown
import numpy as np
import pandas as pd
import psutil
import streamlit as st
import onnxruntime as ort
from numpy.linalg import norm
from sentence_transformers import SentenceTransformer
from transformers import PreTrainedTokenizerFast, AutoTokenizer

# ============================
# Streamlit page config
# ============================
st.set_page_config(page_title="QUANT vs FP32 comparer", layout="wide")
st.title("üîç –°—Ä–∞–≤–Ω–µ–Ω–∏–µ: –æ—Ä–∏–≥–∏–Ω–∞–ª (FP32) vs –∫–≤–∞–Ω—Ç (ONNX INT8) ‚Äî USER-BGE-M3")
st.markdown(
    "–ó–∞–≥—Ä—É–∑–∏—Ç–µ ZIP –∫–≤–∞–Ω—Ç-–º–æ–¥–µ–ª–∏ (Google Drive ID), —É–∫–∞–∂–∏—Ç–µ –ø–∞–ø–∫—É —Ä–∞—Å–ø–∞–∫–æ–≤–∫–∏ –∏ –∑–∞–ø—É—Å—Ç–∏—Ç–µ —Ç–µ—Å—Ç. "
    "–ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –∏–∑–º–µ—Ä–∏—Ç —Å–∫–æ—Ä–æ—Å—Ç—å –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ (CPU) –∏ –∫–∞—á–µ—Å—Ç–≤–æ (cosine similarity) –º–µ–∂–¥—É –º–æ–¥–µ–ª—è–º–∏."
)

# ============================
# UI inputs
# ============================
with st.sidebar:
    st.header("–ù–∞—Å—Ç—Ä–æ–π–∫–∏")
    gdrive_id = st.text_input("Google Drive file ID (zip —Å ONNX)", "")
    quant_dir = st.text_input("–õ–æ–∫–∞–ª—å–Ω–∞—è –ø–∞–ø–∫–∞ –¥–ª—è —Ä–∞—Å–ø–∞–∫–æ–≤–∫–∏ –∫–≤–∞–Ω—Ç–∞", "onnx-quant")
    orig_model_id = st.text_input("–û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å (HF)", "deepvk/USER-bge-m3")
    cpu_only = st.checkbox("–ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ CPU (–¥–ª—è –æ—Ä–∏–≥–∏–Ω–∞–ª–∞)", True)
    warmup_runs = st.number_input("Warmup runs", min_value=0, max_value=10, value=2)
    bench_batch_size = st.number_input("Batch size –¥–ª—è —Ç–µ—Å—Ç–∞ —Å–∫–æ—Ä–æ—Å—Ç–∏", min_value=1, max_value=256, value=32)
    calib_texts_count = st.number_input("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ cosine (samples)", min_value=1, max_value=5000, value=500)
    run_button = st.button("üöÄ –ó–∞–ø—É—Å—Ç–∏—Ç—å —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ")

st.write("**–ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏:** –∑–∞–≥—Ä—É–∑–∏—Ç–µ ZIP (—Å–æ–¥–µ—Ä–∂–∏—Ç model_quantized.onnx –∏ —Ñ–∞–π–ª—ã —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞), "
         "–≤–≤–µ–¥–∏—Ç–µ –µ–≥–æ Google Drive ID —Å–≤–µ—Ä—Ö—É –∏ –Ω–∞–∂–º–∏—Ç–µ `–ó–∞–ø—É—Å—Ç–∏—Ç—å —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ`.")

# ============================
# Helpers
# ============================
def download_and_extract_gdrive(gdrive_id: str, out_dir: Path) -> None:
    out_dir.mkdir(parents=True, exist_ok=True)
    zip_path = out_dir.with_suffix(".zip")
    url = f"https://drive.google.com/uc?id={gdrive_id}"
    st.info(f"–°–∫–∞—á–∏–≤–∞–Ω–∏–µ {url} -> {zip_path.name} ...")
    gdown.download(url, str(zip_path), quiet=False)
    st.info(f"–†–∞—Å–ø–∞–∫–æ–≤–∫–∞ {zip_path.name} ‚Üí {out_dir} ...")
    with zipfile.ZipFile(str(zip_path), "r") as zf:
        zf.extractall(str(out_dir))
    zip_path.unlink(missing_ok=True)

def list_files_recursive(d: Path) -> List[str]:
    res = []
    if not d.exists():
        return res
    for p in d.rglob("*"):
        if p.is_file():
            res.append(str(p.relative_to(d)))
    return res

def flatten_if_nested(model_dir: Path):
    entries = list(model_dir.iterdir())
    if len(entries) == 1 and entries[0].is_dir():
        inner = entries[0]
        for p in inner.iterdir():
            target = model_dir / p.name
            if not target.exists():
                p.rename(target)
        try:
            inner.rmdir()
        except Exception:
            pass

def ensure_tokenizer_filenames(model_dir: Path):
    mappings = {
        "tokenizer": "tokenizer.json",
        "config": "config.json",
        "tokenizer_config": "tokenizer_config.json",
        "special_tokens_map": "special_tokens_map.json",
    }
    for src, dst in mappings.items():
        src_path = model_dir / src
        dst_path = model_dir / dst
        if src_path.exists() and not dst_path.exists():
            try:
                src_path.rename(dst_path)
            except Exception:
                pass

def find_onnx_file(model_dir: Path) -> Optional[Path]:
    onnxs = list(model_dir.rglob("*.onnx"))
    if not onnxs:
        return None
    for f in onnxs:
        if "quant" in f.name.lower():
            return f
    return onnxs[0]

# ============================
# ONNX encoder class
# ============================
class OnnxEncoder:
    def __init__(self, onnx_path: Path, tokenizer):
        self.onnx_path = str(onnx_path)
        so = ort.SessionOptions()
        so.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL
        providers = ["CPUExecutionProvider"]
        self.sess = ort.InferenceSession(self.onnx_path, sess_options=so, providers=providers)
        self.tokenizer = tokenizer

    @staticmethod
    def _mean_pooling(embs: np.ndarray, attention_mask: np.ndarray):
        if embs.ndim == 3:
            mask = attention_mask.astype(np.float32)[..., None]
            summed = (embs * mask).sum(axis=1)
            counts = mask.sum(axis=1)
            counts = np.clip(counts, 1e-9, None)
            return summed / counts
        elif embs.ndim == 2:
            return embs
        else:
            return embs.mean(axis=1)

    def encode_batch(self, texts: List[str], normalize: bool = True) -> np.ndarray:
        inputs = self.tokenizer(texts, padding=True, truncation=True, return_tensors="np")
        # ‚úÖ —Ñ–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —Ç–µ –≤—Ö–æ–¥—ã, –∫–æ—Ç–æ—Ä—ã–µ —Ä–µ–∞–ª—å–Ω–æ –ø—Ä–∏–Ω–∏–º–∞–µ—Ç ONNX
        input_names = {inp.name for inp in self.sess.get_inputs()}
        ort_inputs = {k: v for k, v in inputs.items() if k in input_names}

        outputs = self.sess.run(None, ort_inputs)
        emb = outputs[0]
        pooled = self._mean_pooling(
            emb, ort_inputs.get("attention_mask", np.ones((len(texts), 1)))
        )

        if normalize:
            norms = np.linalg.norm(pooled, axis=1, keepdims=True) + 1e-12
            pooled = pooled / norms
        return pooled

# ============================
# Utilities
# ============================
def cosine_batch(A: np.ndarray, B: np.ndarray) -> np.ndarray:
    A = np.asarray(A)
    B = np.asarray(B)
    if A.shape[1] != B.shape[1]:
        m = min(A.shape[1], B.shape[1])
        A = A[:, :m]
        B = B[:, :m]
    A = A / (np.linalg.norm(A, axis=1, keepdims=True) + 1e-12)
    B = B / (np.linalg.norm(B, axis=1, keepdims=True) + 1e-12)
    return (A * B).sum(axis=1)

def benchmark_encoder(encoder_func, texts: List[str], batch_size: int = 32) -> float:
    start = time.perf_counter()
    for i in range(0, len(texts), batch_size):
        batch = texts[i:i + batch_size]
        _ = encoder_func(batch)
    end = time.perf_counter()
    return end - start

# ============================
# Main runner
# ============================
if run_button:
    try:
        st.info("–ó–∞–ø—É—Å–∫. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π –∏ (–ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏) —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ –∞—Ä—Ö–∏–≤–∞...")
        model_dir = Path(quant_dir)

        if gdrive_id.strip():
            download_and_extract_gdrive(gdrive_id.strip(), model_dir)

        flatten_if_nested(model_dir)
        ensure_tokenizer_filenames(model_dir)

        st.subheader("üìÇ –°–æ–¥–µ—Ä–∂–∏–º–æ–µ –ø–∞–ø–∫–∏ –∫–≤–∞–Ω—Ç-–º–æ–¥–µ–ª–∏")
        files = list_files_recursive(model_dir)
        if not files:
            st.error(f"–ü–∞–ø–∫–∞ {model_dir} –ø—É—Å—Ç–∞ –∏–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø—É—Ç—å/–∞—Ä—Ö–∏–≤.")
            st.stop()
        st.write(files)

        onnx_file = find_onnx_file(model_dir)
        if onnx_file is None:
            st.error("ONNX —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ —É–∫–∞–∑–∞–Ω–Ω–æ–π –ø–∞–ø–∫–µ.")
            st.stop()
        st.success(f"–ù–∞–π–¥–µ–Ω ONNX: {onnx_file.name}")

        tokenizer = None
        used_local_tokenizer = False
        try:
            tokenizer = PreTrainedTokenizerFast.from_pretrained(str(model_dir))
            used_local_tokenizer = True
            st.success(f"–õ–æ–∫–∞–ª—å–Ω—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω –∏–∑ {model_dir}")
        except Exception as e_local:
            st.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –ª–æ–∫–∞–ª—å–Ω—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä: {e_local}")
            try:
                tokenizer = AutoTokenizer.from_pretrained(str(model_dir))
                used_local_tokenizer = True
                st.success("AutoTokenizer —Å–º–æ–≥ –∑–∞–≥—Ä—É–∑–∏—Ç—å —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –ª–æ–∫–∞–ª—å–Ω–æ.")
            except Exception as e2:
                st.warning(f"AutoTokenizer –ª–æ–∫–∞–ª—å–Ω–æ –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª: {e2}")
                st.info("–ë—É–¥–µ–º –ø—ã—Ç–∞—Ç—å—Å—è –≤–∑—è—Ç—å —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∏–∑ HF hub (deepvk/USER-bge-m3).")
                tokenizer = AutoTokenizer.from_pretrained(orig_model_id, use_fast=True)
                used_local_tokenizer = False
                st.success(f"–¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω –∏–∑ HF: {orig_model_id}")

        st.write(f"üîë –ò—Å–ø–æ–ª—å–∑—É–µ–º –ª–æ–∫–∞–ª—å–Ω—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä? {used_local_tokenizer}")

        onnx_encoder = OnnxEncoder(onnx_file, tokenizer)

        device_arg = "cpu" if cpu_only else None
        st.info("–ó–∞–≥—Ä—É–∂–∞–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å –Ω–∞ CPU...")
        if device_arg is not None:
            orig = SentenceTransformer(orig_model_id, device="cpu")
        else:
            orig = SentenceTransformer(orig_model_id)
        st.success("–û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞.")

        st.subheader("üß™ –í–≤–æ–¥ —Ç–µ—Å—Ç–æ–≤—ã—Ö —Ñ—Ä–∞–∑")
        input_texts = st.text_area(
            "–í–≤–µ–¥–∏—Ç–µ —Ç–µ—Å—Ç–æ–≤—ã–µ —Ç–µ–∫—Å—Ç—ã (–ø–æ –æ–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–µ). –û—Å—Ç–∞–≤—å—Ç–µ –ø—É—Å—Ç—ã–º, —á—Ç–æ–±—ã –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏.",
            value="–¢–µ—Å—Ç–æ–≤–∞—è —Å—Ç—Ä–æ–∫–∞ –¥–ª—è –∑–∞–º–µ—Ä–∞ —Å–∫–æ—Ä–æ—Å—Ç–∏.\n–ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–∏.\n–ö–∞–∫ –¥–µ–ª–∞?"
        )
        user_texts = [t.strip() for t in input_texts.splitlines() if t.strip()]
        if not user_texts:
            user_texts = [f"–¢–µ—Å—Ç–æ–≤–∞—è —Å—Ç—Ä–æ–∫–∞ –Ω–æ–º–µ—Ä {i}" for i in range(50)]
        if len(user_texts) < calib_texts_count:
            times = (calib_texts_count + len(user_texts) - 1) // len(user_texts)
            eval_texts = (user_texts * times)[:calib_texts_count]
        else:
            eval_texts = user_texts[:calib_texts_count]

        st.write(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º {len(eval_texts)} —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ cosine similarity –∏ {len(user_texts)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –≤–≤–µ–¥—ë–Ω–Ω—ã—Ö —Ñ—Ä–∞–∑.")

        st.info(f"–ü—Ä–æ–≥—Ä–µ–≤ –º–æ–¥–µ–ª–µ–π ({warmup_runs} –ø—Ä–æ–≥–æ–Ω–æ–≤)...")
        for _ in range(int(warmup_runs)):
            _ = orig.encode(user_texts[:min(8, len(user_texts))], normalize_embeddings=True)
            _ = onnx_encoder.encode_batch(user_texts[:min(8, len(user_texts))], normalize=True)

        st.info("–ò–∑–º–µ—Ä—è–µ–º —Å–∫–æ—Ä–æ—Å—Ç—å –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ (–ø–æ–ª–Ω—ã–π –ø—Ä–æ–≥–æ–Ω)...")
        bench_texts = user_texts * 10
        t0 = time.perf_counter()
        _ = orig.encode(bench_texts, normalize_embeddings=True, batch_size=int(bench_batch_size))
        t1 = time.perf_counter()
        orig_time = t1 - t0
        t0 = time.perf_counter()
        _ = onnx_encoder.encode_batch(bench_texts, normalize=True)
        t1 = time.perf_counter()
        onnx_time = t1 - t0

        st.info("–ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞...")
        emb_orig = orig.encode(eval_texts, normalize_embeddings=True, batch_size=int(bench_batch_size))
        emb_onx = onnx_encoder.encode_batch(eval_texts, normalize=True)

        per_cos = cosine_batch(np.asarray(emb_orig), np.asarray(emb_onx))
        avg_cos = float(np.mean(per_cos))
        med_cos = float(np.median(per_cos))

        def get_dir_size_mb(path: Path) -> float:
            total = 0
            for p in path.rglob("*"):
                if p.is_file():
                    total += p.stat().st_size
            return total / (1024 * 1024)

        quant_size = get_dir_size_mb(model_dir)

        st.subheader("üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å—Ä–∞–≤–Ω–µ–Ω–∏—è")
        metrics = {
            "Metric": ["Avg cosine (orig vs quant)", "Median cosine", "Orig time (s)", "Quant time (s)", "Quant model size (MB)"],
            "Value": [f"{avg_cos:.6f}", f"{med_cos:.6f}", f"{orig_time:.4f}", f"{onnx_time:.4f}", f"{quant_size:.1f}"]
        }
        df = pd.DataFrame(metrics)
        st.table(df)

        col1, col2, col3 = st.columns(3)
        col1.metric("–°—Ä–µ–¥–Ω–µ–µ cosine", f"{avg_cos:.4f}")
        col2.metric("–í—Ä–µ–º—è –æ—Ä–∏–≥–∏–Ω–∞–ª–∞ (s)", f"{orig_time:.3f}")
        col3.metric("–í—Ä–µ–º—è –∫–≤–∞–Ω—Ç–∞ (s)", f"{onnx_time:.3f}")

        st.write("üßæ –î–æ–ø. –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è")
        st.write(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º –ª–æ–∫–∞–ª—å–Ω—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä? {used_local_tokenizer}")
        st.write(f"ONNX —Ñ–∞–π–ª: {onnx_file}")
        st.write("–°–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤ –≤ quant_dir:")
        st.json(files)

        st.success("–ì–æ—Ç–æ–≤–æ ‚úÖ")

    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞: {e}")
        st.text(traceback.format_exc())
