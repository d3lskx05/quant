import os
import zipfile
import time
import traceback
from pathlib import Path
from functools import lru_cache
from typing import Optional

import gdown
import huggingface_hub
import numpy as np
import psutil
import streamlit as st
import onnxruntime as ort
from numpy.linalg import norm
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer

st.set_page_config(page_title="Quantized model tester", layout="wide")

# ============================================================
# üî• QuantModel (–≤—Å—Ç—Ä–æ–µ–Ω —Å—é–¥–∞, —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º –∏ force_download)
# ============================================================
class QuantModel:
    """
    –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –∑–∞–≥—Ä—É–∑—á–∏–∫ –∫–≤–∞–Ω—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö ONNX –º–æ–¥–µ–ª–µ–π.
    –ò—Å—Ç–æ—á–Ω–∏–∫–∏: Google Drive (gdrive), Hugging Face Hub (hf), –ª–æ–∫–∞–ª—å–Ω–∞—è (local).
    –ü–æ–¥–¥–µ—Ä–∂–∫–∞ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è –∏ —Ñ–ª–∞–≥–∞ force_download.
    """

    def __init__(self, model_id: str, source: str = "gdrive",
                 model_dir: str = "onnx_model", tokenizer_name: Optional[str] = None,
                 force_download: bool = False):
        self.model_id = model_id
        self.source = source
        self.model_dir = Path(model_dir)
        self.tokenizer_name = tokenizer_name
        self.force_download = force_download
        self.model_path = None

        self._ensure_model()
        self.session = self._load_session()
        self.tokenizer = self._load_tokenizer()

    def _ensure_model(self):
        """–°–∫–∞—á–∏–≤–∞–Ω–∏–µ –∏ —Ä–∞—Å–ø–∞–∫–æ–≤–∫–∞ –º–æ–¥–µ–ª–∏, —Å —É—á—ë—Ç–æ–º –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è."""
        os.makedirs(self.model_dir, exist_ok=True)

        need_download = self.force_download or not any(self.model_dir.glob("*.onnx"))

        if need_download:
            if self.source == "gdrive":
                zip_path = f"{self.model_dir}.zip"
                print(f"üì• –°–∫–∞—á–∏–≤–∞—é –º–æ–¥–µ–ª—å —Å Google Drive: {self.model_id}")
                gdown.download(f"https://drive.google.com/uc?id={self.model_id}", zip_path, quiet=False)
                print(f"üì¶ –†–∞—Å–ø–∞–∫–æ–≤–∫–∞ {zip_path}...")
                with zipfile.ZipFile(zip_path, "r") as zf:
                    zf.extractall(self.model_dir)
                os.remove(zip_path)

            elif self.source == "hf":
                print(f"üì• –°–∫–∞—á–∏–≤–∞—é –º–æ–¥–µ–ª—å —Å Hugging Face: {self.model_id}")
                huggingface_hub.snapshot_download(
                    repo_id=self.model_id,
                    local_dir=self.model_dir,
                    local_dir_use_symlinks=False,
                    resume_download=True
                )

            elif self.source == "local":
                print(f"üìÇ –ò—Å–ø–æ–ª—å–∑—É—é –ª–æ–∫–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å: {self.model_dir}")
            else:
                raise ValueError(f"‚ùå –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫: {self.source}")
        else:
            print(f"‚úÖ –ò—Å–ø–æ–ª—å–∑—É—é –∑–∞–∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—É—é –º–æ–¥–µ–ª—å –∏–∑ {self.model_dir}")

        onnx_files = list(self.model_dir.rglob("*.onnx"))
        if not onnx_files:
            raise FileNotFoundError(f"‚ùå –í {self.model_dir} –Ω–µ –Ω–∞–π–¥–µ–Ω .onnx —Ñ–∞–π–ª!")
        self.model_path = onnx_files[0]
        print(f"‚úÖ –ù–∞–π–¥–µ–Ω ONNX —Ñ–∞–π–ª: {self.model_path}")

    def _load_session(self):
        so = ort.SessionOptions()
        so.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL
        providers = ["CPUExecutionProvider"]
        try:
            if ort.get_device() == "GPU":
                providers.insert(0, "CUDAExecutionProvider")
        except Exception:
            pass
        print(f"üöÄ –ó–∞–≥—Ä—É–∂–∞—é –º–æ–¥–µ–ª—å –Ω–∞ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞—Ö: {providers}")
        return ort.InferenceSession(str(self.model_path), sess_options=so, providers=providers)

    def _load_tokenizer(self):
        if self.tokenizer_name:
            return AutoTokenizer.from_pretrained(self.tokenizer_name)
        try:
            return AutoTokenizer.from_pretrained(str(self.model_dir))
        except Exception:
            print("‚ö†Ô∏è –¢–æ–∫–µ–Ω–∞–π–∑–µ—Ä –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –ø–∞–ø–∫–µ, –∏—Å–ø–æ–ª—å–∑—É–µ–º deepvk/USER-BGE-M3")
            return AutoTokenizer.from_pretrained("deepvk/USER-BGE-M3")

    @lru_cache(maxsize=1024)
    def _encode_cached(self, text: str, normalize: bool = True):
        inputs = self.tokenizer([text], padding=True, truncation=True, return_tensors="np")
        ort_inputs = {k: v for k, v in inputs.items()}
        embeddings = self.session.run(None, ort_inputs)[0]
        if normalize:
            norms = np.linalg.norm(embeddings, axis=1, keepdims=True) + 1e-10
            embeddings = embeddings / norms
        return embeddings[0]

    def encode(self, texts, normalize=True):
        if isinstance(texts, str):
            texts = [texts]
        return np.array([self._encode_cached(t, normalize) for t in texts])


# ============================================================
# üîß Helpers
# ============================================================
def to_vector(embs):
    """–ü—Ä–µ–≤—Ä–∞—â–∞–µ—Ç –±–∞—Ç—á —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –≤ –æ–¥–∏–Ω –≤–µ–∫—Ç–æ—Ä (—É—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ)."""
    arr = np.array(embs)
    if arr.ndim == 1:
        return arr
    if arr.ndim == 2 and arr.shape[0] == 1:
        return arr[0]
    return arr.mean(axis=0)


def cosine_similarity(vec1, vec2):
    return float(np.dot(vec1, vec2) / ((norm(vec1) * norm(vec2)) + 1e-12))


# ============================================================
# üéõÔ∏è UI
# ============================================================
st.title("üîç –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–π –∏ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–µ–π")

st.markdown(
    """
    - –û—Ä–∏–≥–∏–Ω–∞–ª –≥—Ä—É–∑–∏–º —á–µ—Ä–µ–∑ **SentenceTransformer**.
    - –ö–≤–∞–Ω—Ç–æ–≤–∞–Ω–Ω—É—é –º–æ–¥–µ–ª—å –≥—Ä—É–∑–∏–º —á–µ—Ä–µ–∑ **QuantModel** (onnxruntime, –ø–∞–º—è—Ç—å —ç–∫–æ–Ω–æ–º–∏—Ç—Å—è).
    """
)

col1, col2 = st.columns(2)
with col1:
    st.header("–û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å")
    orig_id = st.text_input("HF repo ID –∏–ª–∏ –ª–æ–∫–∞–ª—å–Ω—ã–π –ø—É—Ç—å", "deepvk/USER-BGE-M3")

with col2:
    st.header("–ö–≤–∞–Ω—Ç–æ–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å")
    quant_source = st.selectbox("–ò—Å—Ç–æ—á–Ω–∏–∫", ["gdrive", "hf", "local"], index=1)
    quant_id = st.text_input("ID/Repo/Path", "1lkrvCPIE1wvffIuCSHGtbEz3Epjx5R36")
    quant_dir = st.text_input("–ü–∞–ø–∫–∞ –¥–ª—è –∫–≤–∞–Ω—Ç–∞ (–ª–æ–∫–∞–ª—å–Ω–∞—è)", "onnx-user-bge-m3")
    tokenizer_name = st.text_input("Tokenizer name (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)", "")
    force_download = st.checkbox("‚ôªÔ∏è –ü–µ—Ä–µ–∫–∞—á–∞—Ç—å –º–æ–¥–µ–ª—å –∑–∞–Ω–æ–≤–æ", False)

st.markdown("---")
input_text = st.text_area("–¢–µ–∫—Å—Ç—ã –¥–ª—è —Ç–µ—Å—Ç–∞ (–ø–æ –æ–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–µ)", "–≠—Ç–æ —Ç–µ—Å—Ç–æ–≤–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ.\n–ü—Ä–∏–º–µ—Ä –≤—Ç–æ—Ä–æ–π —Å—Ç—Ä–æ–∫–∏.")
texts = [t.strip() for t in input_text.split("\n") if t.strip()]

batch_size = st.slider("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–≤—Ç–æ—Ä–æ–≤ –¥–ª—è throughput-—Ç–µ—Å—Ç–∞", 1, 128, 8)
run_button = st.button("üöÄ –ó–∞–ø—É—Å—Ç–∏—Ç—å —Ç–µ—Å—Ç")

# ============================================================
# üöÄ –û—Å–Ω–æ–≤–Ω–∞—è –ª–æ–≥–∏–∫–∞
# ============================================================
if run_button:
    try:
        proc = psutil.Process()

        # --- –ó–∞–≥—Ä—É–∂–∞–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª —á–µ—Ä–µ–∑ SentenceTransformer
        with st.spinner("–ó–∞–≥—Ä—É–∑–∫–∞ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏..."):
            orig_model = SentenceTransformer(orig_id)
            st.success(f"‚úÖ –û—Ä–∏–≥–∏–Ω–∞–ª –∑–∞–≥—Ä—É–∂–µ–Ω –∏–∑ {orig_id}")

        # --- –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–≤–∞–Ω—Ç —á–µ—Ä–µ–∑ QuantModel
        with st.spinner("–ó–∞–≥—Ä—É–∑–∫–∞ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏..."):
            quant_model = QuantModel(
                model_id=quant_id,
                source=quant_source,
                model_dir=quant_dir,
                tokenizer_name=tokenizer_name if tokenizer_name else None,
                force_download=force_download
            )
            st.success(f"‚úÖ –ö–≤–∞–Ω—Ç–æ–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ ({quant_model.model_path})")

        texts_for_run = (texts * batch_size)[:max(len(texts), 1)]

        # –û—Ä–∏–≥–∏–Ω–∞–ª
        t0 = time.perf_counter()
        orig_embs = orig_model.encode(texts_for_run, normalize_embeddings=True)
        t1 = time.perf_counter()
        orig_time = t1 - t0
        mem_after_orig = proc.memory_info().rss / 1024 ** 2

        # –ö–≤–∞–Ω—Ç
        t0 = time.perf_counter()
        quant_embs = quant_model.encode(texts_for_run, normalize=True)
        t1 = time.perf_counter()
        quant_time = t1 - t0
        mem_after_quant = proc.memory_info().rss / 1024 ** 2

        v_orig = to_vector(orig_embs)
        v_quant = to_vector(quant_embs)
        if v_orig.shape != v_quant.shape:
            st.warning(f"–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ —Ä–∞–∑–ª–∏—á–∞—é—Ç—Å—è: {v_orig.shape} vs {v_quant.shape}, —É—Å–µ—á–µ–º –¥–æ min.")
            m = min(v_orig.size, v_quant.size)
            v_orig = v_orig[:m]
            v_quant = v_quant[:m]

        cos = cosine_similarity(v_orig, v_quant)

        # –í—ã–≤–æ–¥
        st.subheader("üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã")
        st.metric("Latency Original (s)", f"{orig_time:.4f}")
        st.metric("Latency Quant (s)", f"{quant_time:.4f}")
        st.metric("Cosine Similarity", f"{cos:.4f}")
        st.write(f"Memory after original: {mem_after_orig:.1f} MB")
        st.write(f"Memory after quant: {mem_after_quant:.1f} MB")

        st.bar_chart({
            "Latency (s)": [orig_time, quant_time],
            "Memory (MB)": [mem_after_orig, mem_after_quant]
        })

    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞: {e}")
        st.text(traceback.format_exc())
