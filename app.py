# app.py
import os
import zipfile
import time
import traceback
from pathlib import Path
from functools import lru_cache
from typing import Optional, List

import gdown
import numpy as np
import pandas as pd
import psutil
import streamlit as st
import onnxruntime as ort
from numpy.linalg import norm
from sentence_transformers import SentenceTransformer
from transformers import PreTrainedTokenizerFast, AutoTokenizer

# ============================
# Streamlit page config
# ============================
st.set_page_config(page_title="QUANT vs FP32 comparer", layout="wide")
st.title("üîç –°—Ä–∞–≤–Ω–µ–Ω–∏–µ: –æ—Ä–∏–≥–∏–Ω–∞–ª (FP32) vs –∫–≤–∞–Ω—Ç (ONNX INT8) ‚Äî USER-BGE-M3")
st.markdown(
    "–ó–∞–≥—Ä—É–∑–∏—Ç–µ ZIP –∫–≤–∞–Ω—Ç-–º–æ–¥–µ–ª–∏ (Google Drive ID), —É–∫–∞–∂–∏—Ç–µ –ø–∞–ø–∫—É —Ä–∞—Å–ø–∞–∫–æ–≤–∫–∏ –∏ –∑–∞–ø—É—Å—Ç–∏—Ç–µ —Ç–µ—Å—Ç. "
    "–ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –∏–∑–º–µ—Ä–∏—Ç —Å–∫–æ—Ä–æ—Å—Ç—å –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ (CPU) –∏ –∫–∞—á–µ—Å—Ç–≤–æ (cosine similarity) –º–µ–∂–¥—É –º–æ–¥–µ–ª—è–º–∏."
)

# ============================
# UI inputs
# ============================
with st.sidebar:
    st.header("–ù–∞—Å—Ç—Ä–æ–π–∫–∏")
    gdrive_id = st.text_input("Google Drive file ID (zip —Å ONNX)", "")
    quant_dir = st.text_input("–õ–æ–∫–∞–ª—å–Ω–∞—è –ø–∞–ø–∫–∞ –¥–ª—è —Ä–∞—Å–ø–∞–∫–æ–≤–∫–∏ –∫–≤–∞–Ω—Ç–∞", "onnx-quant")
    orig_model_id = st.text_input("–û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å (HF)", "deepvk/USER-bge-m3")
    cpu_only = st.checkbox("–ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ CPU (–¥–ª—è –æ—Ä–∏–≥–∏–Ω–∞–ª–∞)", True)
    warmup_runs = st.number_input("Warmup runs", min_value=0, max_value=10, value=2)
    bench_batch_size = st.number_input("Batch size –¥–ª—è —Ç–µ—Å—Ç–∞ —Å–∫–æ—Ä–æ—Å—Ç–∏", min_value=1, max_value=256, value=32)
    calib_texts_count = st.number_input("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ cosine (samples)", min_value=1, max_value=5000, value=500)
    run_button = st.button("üöÄ –ó–∞–ø—É—Å—Ç–∏—Ç—å —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ")

st.write("**–ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏:** –∑–∞–≥—Ä—É–∑–∏—Ç–µ ZIP (—Å–æ–¥–µ—Ä–∂–∏—Ç model_quantized.onnx –∏ —Ñ–∞–π–ª—ã —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞), "
         "–≤–≤–µ–¥–∏—Ç–µ –µ–≥–æ Google Drive ID —Å–≤–µ—Ä—Ö—É –∏ –Ω–∞–∂–º–∏—Ç–µ `–ó–∞–ø—É—Å—Ç–∏—Ç—å —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ`.")

# ============================
# Helpers: filesystem / unzip / fix names
# ============================
def download_and_extract_gdrive(gdrive_id: str, out_dir: Path) -> None:
    """–°–∫–∞—á–∞—Ç—å zip —Å gdrive (—á–µ—Ä–µ–∑ –ø—Ä—è–º—É—é —Å—Å—ã–ª–∫—É) –∏ —Ä–∞—Å–ø–∞–∫–æ–≤–∞—Ç—å –≤ out_dir."""
    out_dir.mkdir(parents=True, exist_ok=True)
    zip_path = out_dir.with_suffix(".zip")
    url = f"https://drive.google.com/uc?id={gdrive_id}"
    st.info(f"–°–∫–∞—á–∏–≤–∞–Ω–∏–µ {url} -> {zip_path.name} ...")
    gdown.download(url, str(zip_path), quiet=False)
    st.info(f"–†–∞—Å–ø–∞–∫–æ–≤–∫–∞ {zip_path.name} ‚Üí {out_dir} ...")
    with zipfile.ZipFile(str(zip_path), "r") as zf:
        # –†–∞—Å–ø–∞–∫—É–µ–º –≤—Å–µ –≤ –≤—Ä–µ–º–µ–Ω–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é, –∑–∞—Ç–µ–º –ø–æ–ø—Ä–æ–±—É–µ–º –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä—É
        zf.extractall(str(out_dir))
    zip_path.unlink(missing_ok=True)

def list_files_recursive(d: Path) -> List[str]:
    res = []
    if not d.exists():
        return res
    for p in d.rglob("*"):
        if p.is_file():
            res.append(str(p.relative_to(d)))
    return res

def flatten_if_nested(model_dir: Path):
    """
    –ò–Ω–æ–≥–¥–∞ –∞—Ä—Ö–∏–≤ —Å–æ–¥–µ—Ä–∂–∏—Ç –≤–ª–æ–∂–µ–Ω–Ω—É—é –ø–∞–ø–∫—É with same name.
    –ï—Å–ª–∏ –≤–Ω—É—Ç—Ä–∏ model_dir –µ—Å—Ç—å —Ä–æ–≤–Ω–æ –æ–¥–Ω–∞ –ø–æ–¥–ø–∞–ø–∫–∞, –∏ –≤ –Ω–µ–π –ª–µ–∂–∞—Ç –Ω—É–∂–Ω—ã–µ —Ñ–∞–π–ª—ã, –ø–µ—Ä–µ–º–µ—Å—Ç–∏—Ç—å –∏—Ö –≤–≤–µ—Ä—Ö.
    """
    entries = list(model_dir.iterdir())
    # –µ—Å–ª–∏ –≤–Ω—É—Ç—Ä–∏ –æ–¥–Ω–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –∏ –≤–Ω—É—Ç—Ä–∏ –Ω–µ—ë –º–Ω–æ–≥–æ —Ñ–∞–π–ª–æ–≤ ‚Äî –ø–µ—Ä–µ–º–µ—â–∞–µ–º
    if len(entries) == 1 and entries[0].is_dir():
        inner = entries[0]
        # move contents of inner to model_dir
        for p in inner.iterdir():
            target = model_dir / p.name
            if target.exists():
                # –µ—Å–ª–∏ —Ñ–∞–π–ª —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, –ø—Ä–æ–ø—É—Å—Ç–∏—Ç—å
                continue
            p.rename(target)
        # —É–¥–∞–ª–∏–º –ø—É—Å—Ç—É—é –ø–∞–ø–∫—É
        try:
            inner.rmdir()
        except Exception:
            pass

def ensure_tokenizer_filenames(model_dir: Path):
    """
    –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞—Ç—å —Ñ–∞–π–ª—ã: tokenizer -> tokenizer.json, config -> config.json –∏ —Ç.–ø.
    (–Ω–∞ —Å–ª—É—á–∞–π, –µ—Å–ª–∏ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ —Å–∫—Ä—ã—Ç–æ/–æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç).
    """
    mappings = {
        "tokenizer": "tokenizer.json",
        "config": "config.json",
        "tokenizer_config": "tokenizer_config.json",
        "special_tokens_map": "special_tokens_map.json",
    }
    for src, dst in mappings.items():
        src_path = model_dir / src
        dst_path = model_dir / dst
        # –ï—Å–ª–∏ src exists and dst does not, rename
        if src_path.exists() and not dst_path.exists():
            try:
                src_path.rename(dst_path)
            except Exception:
                # –∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º –æ—à–∏–±–∫–∏ –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–∏—è
                pass

def find_onnx_file(model_dir: Path) -> Optional[Path]:
    onnxs = list(model_dir.rglob("*.onnx"))
    if not onnxs:
        return None
    # Preference: file with 'quant' in name
    for f in onnxs:
        if "quant" in f.name.lower():
            return f
    return onnxs[0]

# ============================
# ONNX encoder class
# ============================
class OnnxEncoder:
    def __init__(self, onnx_path: Path, tokenizer):
        self.onnx_path = str(onnx_path)
        so = ort.SessionOptions()
        so.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL
        providers = ["CPUExecutionProvider"]
        self.sess = ort.InferenceSession(self.onnx_path, sess_options=so, providers=providers)
        self.tokenizer = tokenizer

    @staticmethod
    def _mean_pooling(embs: np.ndarray, attention_mask: np.ndarray):
        # embs: (batch, seq, dim) or (batch, dim)
        if embs.ndim == 3:
            mask = attention_mask.astype(np.float32)[..., None]  # (batch, seq, 1)
            summed = (embs * mask).sum(axis=1)  # (batch, dim)
            counts = mask.sum(axis=1)  # (batch, 1)
            counts = np.clip(counts, 1e-9, None)
            return summed / counts
        elif embs.ndim == 2:
            return embs
        else:
            # fallback
            return embs.mean(axis=1)

    def encode_batch(self, texts: List[str], normalize: bool = True) -> np.ndarray:
        # tokenizer from transformers supports return_tensors="np"
        inputs = self.tokenizer(texts, padding=True, truncation=True, return_tensors="np")
        ort_inputs = {k: v for k, v in inputs.items()}
        outputs = self.sess.run(None, ort_inputs)
        emb = outputs[0]  # assume first output is token embeddings or sentence embeddings
        pooled = self._mean_pooling(emb, ort_inputs.get("attention_mask", np.ones((len(texts), 1))))
        if normalize:
            norms = np.linalg.norm(pooled, axis=1, keepdims=True) + 1e-12
            pooled = pooled / norms
        return pooled

# ============================
# Utilities for measurements
# ============================
def cosine_batch(A: np.ndarray, B: np.ndarray) -> np.ndarray:
    A = np.asarray(A)
    B = np.asarray(B)
    # align dims
    if A.shape[1] != B.shape[1]:
        m = min(A.shape[1], B.shape[1])
        A = A[:, :m]
        B = B[:, :m]
    A = A / (np.linalg.norm(A, axis=1, keepdims=True) + 1e-12)
    B = B / (np.linalg.norm(B, axis=1, keepdims=True) + 1e-12)
    return (A * B).sum(axis=1)

def benchmark_encoder(encoder_func, texts: List[str], batch_size: int = 32) -> float:
    start = time.perf_counter()
    # single pass encode
    for i in range(0, len(texts), batch_size):
        batch = texts[i:i + batch_size]
        _ = encoder_func(batch)
    end = time.perf_counter()
    return end - start

# ============================
# Main runner
# ============================
if run_button:
    try:
        st.info("–ó–∞–ø—É—Å–∫. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π –∏ (–ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏) —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ –∞—Ä—Ö–∏–≤–∞...")
        model_dir = Path(quant_dir)

        # 1) download & extract if gdrive id provided
        if gdrive_id.strip():
            download_and_extract_gdrive(gdrive_id.strip(), model_dir)

        # 2) flatten nested and ensure tokenizer files
        flatten_if_nested(model_dir)
        ensure_tokenizer_filenames(model_dir)

        # 3) show files
        st.subheader("üìÇ –°–æ–¥–µ—Ä–∂–∏–º–æ–µ –ø–∞–ø–∫–∏ –∫–≤–∞–Ω—Ç-–º–æ–¥–µ–ª–∏")
        files = list_files_recursive(model_dir)
        if not files:
            st.error(f"–ü–∞–ø–∫–∞ {model_dir} –ø—É—Å—Ç–∞ –∏–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø—É—Ç—å/–∞—Ä—Ö–∏–≤.")
            st.stop()
        st.write(files)

        # 4) find onnx
        onnx_file = find_onnx_file(model_dir)
        if onnx_file is None:
            st.error("ONNX —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ —É–∫–∞–∑–∞–Ω–Ω–æ–π –ø–∞–ø–∫–µ.")
            st.stop()
        st.success(f"–ù–∞–π–¥–µ–Ω ONNX: {onnx_file.name}")

        # 5) try load local tokenizer (preferred)
        tokenizer = None
        used_local_tokenizer = False
        try:
            tokenizer = PreTrainedTokenizerFast.from_pretrained(str(model_dir))
            used_local_tokenizer = True
            st.success(f"–õ–æ–∫–∞–ª—å–Ω—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω –∏–∑ {model_dir}")
        except Exception as e_local:
            st.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –ª–æ–∫–∞–ª—å–Ω—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä: {e_local}")
            # try AutoTokenizer fallback to HF id (but don't do if user explicitly wants local-only)
            try:
                tokenizer = AutoTokenizer.from_pretrained(str(model_dir))
                used_local_tokenizer = True
                st.success("AutoTokenizer —Å–º–æ–≥ –∑–∞–≥—Ä—É–∑–∏—Ç—å —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –ª–æ–∫–∞–ª—å–Ω–æ.")
            except Exception as e2:
                st.warning(f"AutoTokenizer –ª–æ–∫–∞–ª—å–Ω–æ –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª: {e2}")
                st.info("–ë—É–¥–µ–º –ø—ã—Ç–∞—Ç—å—Å—è –≤–∑—è—Ç—å —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∏–∑ HF hub (deepvk/USER-bge-m3).")
                tokenizer = AutoTokenizer.from_pretrained(orig_model_id, use_fast=True)
                used_local_tokenizer = False
                st.success(f"–¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω –∏–∑ HF: {orig_model_id}")

        st.write(f"üîë –ò—Å–ø–æ–ª—å–∑—É–µ–º –ª–æ–∫–∞–ª—å–Ω—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä? {used_local_tokenizer}")

        # 6) build ONNX encoder
        onnx_encoder = OnnxEncoder(onnx_file, tokenizer)

        # 7) load original SentenceTransformer on CPU (force CPU to avoid meta-tensor problems)
        device_arg = "cpu" if cpu_only else None
        st.info("–ó–∞–≥—Ä—É–∂–∞–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å –Ω–∞ CPU...")
        # SentenceTransformer may download model the first time; show info
        if device_arg is not None:
            orig = SentenceTransformer(orig_model_id, device="cpu")
        else:
            orig = SentenceTransformer(orig_model_id)
        st.success("–û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞.")

        # 8) prepare test texts
        # use user-entered texts or sample set
        st.subheader("üß™ –í–≤–æ–¥ —Ç–µ—Å—Ç–æ–≤—ã—Ö —Ñ—Ä–∞–∑")
        input_texts = st.text_area("–í–≤–µ–¥–∏—Ç–µ —Ç–µ—Å—Ç–æ–≤—ã–µ —Ç–µ–∫—Å—Ç—ã (–ø–æ –æ–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–µ). –û—Å—Ç–∞–≤—å—Ç–µ –ø—É—Å—Ç—ã–º, —á—Ç–æ–±—ã –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏.",
                                   value="–¢–µ—Å—Ç–æ–≤–∞—è —Å—Ç—Ä–æ–∫–∞ –¥–ª—è –∑–∞–º–µ—Ä–∞ —Å–∫–æ—Ä–æ—Å—Ç–∏.\n–ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–∏.\n–ö–∞–∫ –¥–µ–ª–∞?")
        user_texts = [t.strip() for t in input_texts.splitlines() if t.strip()]
        if not user_texts:
            # fallback sample
            user_texts = [f"–¢–µ—Å—Ç–æ–≤–∞—è —Å—Ç—Ä–æ–∫–∞ –Ω–æ–º–µ—Ä {i}" for i in range(50)]
        # expand to desired sample count for cosine evaluation
        if len(user_texts) < calib_texts_count:
            # repeat to reach target size
            times = (calib_texts_count + len(user_texts) - 1) // len(user_texts)
            eval_texts = (user_texts * times)[:calib_texts_count]
        else:
            eval_texts = user_texts[:calib_texts_count]

        st.write(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º {len(eval_texts)} —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ cosine similarity –∏ {len(user_texts)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –≤–≤–µ–¥—ë–Ω–Ω—ã—Ö —Ñ—Ä–∞–∑.")

        # 9) warmup runs
        st.info(f"–ü—Ä–æ–≥—Ä–µ–≤ –º–æ–¥–µ–ª–µ–π ({warmup_runs} –ø—Ä–æ–≥–æ–Ω–æ–≤)...")
        for _ in range(int(warmup_runs)):
            _ = orig.encode(user_texts[:min(8, len(user_texts))], normalize_embeddings=True)
            _ = onnx_encoder.encode_batch(user_texts[:min(8, len(user_texts))], normalize=True)

        # 10) benchmark speed
        st.info("–ò–∑–º–µ—Ä—è–µ–º —Å–∫–æ—Ä–æ—Å—Ç—å –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ (–ø–æ–ª–Ω—ã–π –ø—Ä–æ–≥–æ–Ω)...")
        bench_texts = user_texts * 10  # make it longer to smooth variability
        # time original
        t0 = time.perf_counter()
        _ = orig.encode(bench_texts, normalize_embeddings=True, batch_size=int(bench_batch_size))
        t1 = time.perf_counter()
        orig_time = t1 - t0
        # time onnx
        t0 = time.perf_counter()
        _ = onnx_encoder.encode_batch(bench_texts, normalize=True)
        t1 = time.perf_counter()
        onnx_time = t1 - t0

        # 11) compute embeddings for evaluation texts (normalized)
        st.info("–ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞...")
        emb_orig = orig.encode(eval_texts, normalize_embeddings=True, batch_size=int(bench_batch_size))
        emb_onx = onnx_encoder.encode_batch(eval_texts, normalize=True)

        # 12) cosine per-sample
        per_cos = cosine_batch(np.asarray(emb_orig), np.asarray(emb_onx))
        avg_cos = float(np.mean(per_cos))
        med_cos = float(np.median(per_cos))

        # 13) sizes
        def get_dir_size_mb(path: Path) -> float:
            total = 0
            for p in path.rglob("*"):
                if p.is_file():
                    total += p.stat().st_size
            return total / (1024 * 1024)

        orig_size = None
        try:
            # if HF local cache exists, try to get model size from cache directory
            orig_size = "n/a"
        except Exception:
            orig_size = "n/a"
        quant_size = get_dir_size_mb(model_dir)

        # 14) show results
        st.subheader("üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å—Ä–∞–≤–Ω–µ–Ω–∏—è")
        metrics = {
            "Metric": ["Avg cosine (orig vs quant)", "Median cosine", "Orig time (s)", "Quant time (s)", "Quant model size (MB)"],
            "Value": [f"{avg_cos:.6f}", f"{med_cos:.6f}", f"{orig_time:.4f}", f"{onnx_time:.4f}", f"{quant_size:.1f}"]
        }
        df = pd.DataFrame(metrics)
        st.table(df)

        # nice big numbers
        col1, col2, col3 = st.columns(3)
        col1.metric("–°—Ä–µ–¥–Ω–µ–µ cosine", f"{avg_cos:.4f}")
        col2.metric("–í—Ä–µ–º—è –æ—Ä–∏–≥–∏–Ω–∞–ª–∞ (s)", f"{orig_time:.3f}")
        col3.metric("–í—Ä–µ–º—è –∫–≤–∞–Ω—Ç–∞ (s)", f"{onnx_time:.3f}")

        st.write("üßæ –î–æ–ø. –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è")
        st.write(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º –ª–æ–∫–∞–ª—å–Ω—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä? {used_local_tokenizer}")
        st.write(f"ONNX —Ñ–∞–π–ª: {onnx_file}")
        st.write("–°–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤ –≤ quant_dir:")
        st.json(files)

        st.success("–ì–æ—Ç–æ–≤–æ ‚úÖ")

    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞: {e}")
        st.text(traceback.format_exc())
