import os
import zipfile
import time
import traceback
from pathlib import Path
from functools import lru_cache
from typing import Optional

import gdown
import huggingface_hub
import numpy as np
import psutil
import pandas as pd
import streamlit as st
import onnxruntime as ort
from numpy.linalg import norm
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer

st.set_page_config(page_title="Quantized model tester", layout="wide")

# ============================================================
# üî• QuantModel
# –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –∑–∞–≥—Ä—É–∑—á–∏–∫ –∫–≤–∞–Ω—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö ONNX –º–æ–¥–µ–ª–µ–π.
# ============================================================
class QuantModel:
    def __init__(self, model_id: str, source: str = "gdrive",
                 model_dir: str = "onnx_model", tokenizer_name: Optional[str] = None,
                 force_download: bool = False):
        self.model_id = model_id
        self.source = source
        self.model_dir = Path(model_dir)
        self.tokenizer_name = tokenizer_name
        self.force_download = force_download
        self.model_path = None

        self._ensure_model()
        self.session = self._load_session()
        self.tokenizer = self._load_tokenizer()

    def _ensure_model(self):
        os.makedirs(self.model_dir, exist_ok=True)
        need_download = self.force_download or not any(self.model_dir.rglob("*.onnx"))
        if need_download:
            if self.source == "gdrive":
                zip_path = f"{self.model_dir}.zip"
                gdown.download(f"https://drive.google.com/uc?id={self.model_id}", zip_path, quiet=False)
                with zipfile.ZipFile(zip_path, "r") as zf:
                    zf.extractall(self.model_dir)
                os.remove(zip_path)
            elif self.source == "hf":
                huggingface_hub.snapshot_download(
                    repo_id=self.model_id,
                    local_dir=self.model_dir,
                    local_dir_use_symlinks=False,
                    resume_download=True
                )
            elif self.source == "local":
                pass
            else:
                raise ValueError(f"‚ùå –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫: {self.source}")
        onnx_files = list(self.model_dir.rglob("*.onnx"))
        if not onnx_files:
            raise FileNotFoundError(f"‚ùå –ù–µ—Ç .onnx –º–æ–¥–µ–ª–∏ –≤ {self.model_dir}")
        self.model_path = onnx_files[0]
        for f in onnx_files:
            if "quant" in f.name.lower():
                self.model_path = f
                break

    def _load_session(self):
        so = ort.SessionOptions()
        so.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL
        providers = ["CPUExecutionProvider"]
        try:
            if ort.get_device() == "GPU":
                providers.insert(0, "CUDAExecutionProvider")
        except Exception:
            pass
        return ort.InferenceSession(str(self.model_path), sess_options=so, providers=providers)

    def _load_tokenizer(self):
        if self.tokenizer_name:
            tok = AutoTokenizer.from_pretrained(self.tokenizer_name, use_fast=True)
            print(f"üîë Tokenizer –∑–∞–≥—Ä—É–∂–µ–Ω –∏–∑ repo/id: {self.tokenizer_name}")
            return tok
        try:
            tok = AutoTokenizer.from_pretrained(str(self.model_dir), use_fast=True)
            print(f"üîë Tokenizer –Ω–∞–π–¥–µ–Ω –ª–æ–∫–∞–ª—å–Ω–æ –≤ {self.model_dir}")
            return tok
        except Exception:
            tok = AutoTokenizer.from_pretrained("deepvk/USER-BGE-M3", use_fast=True)
            print("‚ö†Ô∏è Tokenizer –Ω–µ –Ω–∞–π–¥–µ–Ω –ª–æ–∫–∞–ª—å–Ω–æ, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω deepvk/USER-BGE-M3")
            return tok

    @lru_cache(maxsize=1024)
    def _encode_cached(self, text: str, normalize: bool = True):
        inputs = self.tokenizer([text], padding=True, truncation=True, return_tensors="np")
        ort_inputs = {k: v for k, v in inputs.items()}
        outputs = self.session.run(None, ort_inputs)
        embeddings = outputs[0]
        if embeddings.ndim == 3:
            mask = ort_inputs["attention_mask"].astype(np.float32)  # (batch, seq)
            embeddings = (embeddings * mask[..., None]).sum(1) \
                         / np.clip(mask.sum(1, keepdims=True), 1e-6, None)
        if normalize:
            norms = np.linalg.norm(embeddings, axis=1, keepdims=True) + 1e-12
            embeddings = embeddings / norms
        return embeddings[0]

    def encode(self, texts, normalize=True):
        if isinstance(texts, str):
            texts = [texts]
        return np.array([self._encode_cached(t, normalize) for t in texts])

# ============================================================
# üîß –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏
# ============================================================
def cosine_batch(A, B):
    A = np.asarray(A)
    B = np.asarray(B)
    if A.shape != B.shape:
        m = min(A.shape[-1], B.shape[-1])
        A = A[..., :m]
        B = B[..., :m]
    A = A / (np.linalg.norm(A, axis=1, keepdims=True) + 1e-12)
    B = B / (np.linalg.norm(B, axis=1, keepdims=True) + 1e-12)
    return (A * B).sum(axis=1)

# ============================================================
# üéõÔ∏è UI
# ============================================================
st.title("üîç –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–µ–π: –û—Ä–∏–≥–∏–Ω–∞–ª vs –ö–≤–∞–Ω—Ç")
mode = st.radio("–í—ã–±–µ—Ä–∏—Ç–µ —Ä–µ–∂–∏–º:", ["–û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å", "–ö–≤–∞–Ω—Ç–æ–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å", "–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –æ–±–µ–∏—Ö"])

if st.button("‚ôªÔ∏è –°–±—Ä–æ—Å–∏—Ç—å —Å–µ—Å—Å–∏—é"):
    st.cache_data.clear()
    st.cache_resource.clear()
    st.rerun()

input_text = st.text_area("–¢–µ–∫—Å—Ç—ã –¥–ª—è —Ç–µ—Å—Ç–∞ (–ø–æ –æ–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–µ)", 
                          "–≠—Ç–æ —Ç–µ—Å—Ç–æ–≤–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ.\n–ü—Ä–∏–º–µ—Ä –≤—Ç–æ—Ä–æ–π —Å—Ç—Ä–æ–∫–∏.")
texts = [t.strip() for t in input_text.split("\n") if t.strip()]
batch_size = st.slider("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–≤—Ç–æ—Ä–æ–≤ –¥–ª—è throughput-—Ç–µ—Å—Ç–∞", 1, 128, 8)
force_download = st.checkbox("‚ôªÔ∏è –ü–µ—Ä–µ–∫–∞—á–∞—Ç—å –∫–≤–∞–Ω—Ç-–º–æ–¥–µ–ª—å –∑–∞–Ω–æ–≤–æ", False)

metrics_df = None

if mode == "–û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å":
    model_id = st.text_input("HF repo ID", "deepvk/USER-BGE-M3")

elif mode == "–ö–≤–∞–Ω—Ç–æ–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å":
    col1, col2 = st.columns(2)
    with col1:
        quant_source = st.selectbox("–ò—Å—Ç–æ—á–Ω–∏–∫", ["gdrive", "hf", "local"], index=1)
        quant_id = st.text_input("ID/Repo/Path", "1ym0Lb_1C0p0QSIEMOmFIFaGGtCk7JNO5")
    with col2:
        quant_dir = st.text_input("–ü–∞–ø–∫–∞ –¥–ª—è –∫–≤–∞–Ω—Ç–∞", "onnx-user-bge-m3")
        tokenizer_name = st.text_input("Tokenizer name", "")

else:  # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –æ–±–µ–∏—Ö
    st.markdown("–í —ç—Ç–æ–º —Ä–µ–∂–∏–º–µ –∏–∑–º–µ—Ä—è–µ–º **–∫–∞—á–µ—Å—Ç–≤–æ (cosine similarity)** –∏ **—Å–∫–æ—Ä–æ—Å—Ç—å**.")
    col1, col2 = st.columns(2)
    with col1:
        model_id = st.text_input("HF repo ID (–æ—Ä–∏–≥–∏–Ω–∞–ª)", "deepvk/USER-BGE-M3", key="orig_repo_cmp")
    with col2:
        quant_source = st.selectbox("–ò—Å—Ç–æ—á–Ω–∏–∫ –∫–≤–∞–Ω—Ç–∞", ["gdrive", "hf", "local"], 
                                    index=1, key="quant_src_cmp")
        quant_id = st.text_input("ID/Repo/Path (–∫–≤–∞–Ω—Ç)", 
                                 "1ym0Lb_1C0p0QSIEMOmFIFaGGtCk7JNO5", key="quant_id_cmp")
    col3, col4 = st.columns(2)
    with col3:
        quant_dir = st.text_input("–ü–∞–ø–∫–∞ –¥–ª—è –∫–≤–∞–Ω—Ç–∞", 
                                  "onnx-user-bge-m3-quantized-dyn", key="quant_dir_cmp")
    with col4:
        tokenizer_name = st.text_input("Tokenizer name", "", key="tok_cmp")

run_button = st.button("üöÄ –ó–∞–ø—É—Å—Ç–∏—Ç—å —Ç–µ—Å—Ç")

# ============================================================
# üöÄ –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–∞
# ============================================================
if run_button:
    try:
        texts_for_run = texts * batch_size

        if mode == "–û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å":
            proc = psutil.Process()
            with st.spinner("–ó–∞–≥—Ä—É–∑–∫–∞ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏..."):
                model = SentenceTransformer(model_id)
            t0 = time.perf_counter()
            embs = model.encode(texts_for_run, normalize_embeddings=True)
            t1 = time.perf_counter()
            latency = t1 - t0
            memory = proc.memory_info().rss / 1024 ** 2

            metrics_df = pd.DataFrame([{
                "Mode": "Original",
                "Batch Size": batch_size,
                "Latency (s)": latency,
                "Throughput (texts/s)": len(texts_for_run) / max(latency, 1e-12),
                "Memory (MB)": memory
            }])

            st.subheader("üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã")
            st.dataframe(metrics_df)

        elif mode == "–ö–≤–∞–Ω—Ç–æ–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å":
            proc = psutil.Process()
            with st.spinner("–ó–∞–≥—Ä—É–∑–∫–∞ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏..."):
                model = QuantModel(
                    model_id=quant_id,
                    source=quant_source,
                    model_dir=quant_dir,
                    tokenizer_name=tokenizer_name if tokenizer_name else None,
                    force_download=force_download
                )
            st.write(f"üîë –ò—Å–ø–æ–ª—å–∑—É–µ–º—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä: `{model.tokenizer.name_or_path}`")

            t0 = time.perf_counter()
            embs = model.encode(texts_for_run, normalize=True)
            t1 = time.perf_counter()
            latency = t1 - t0
            memory = proc.memory_info().rss / 1024 ** 2

            metrics_df = pd.DataFrame([{
                "Mode": "Quantized",
                "Batch Size": batch_size,
                "Latency (s)": latency,
                "Throughput (texts/s)": len(texts_for_run) / max(latency, 1e-12),
                "Memory (MB)": memory
            }])

            st.subheader("üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã")
            st.dataframe(metrics_df)

        else:  # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –æ–±–µ–∏—Ö
            with st.spinner("–ó–∞–≥—Ä—É–∑–∫–∞ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏..."):
                orig = SentenceTransformer(model_id)
            t0 = time.perf_counter()
            orig_embs = orig.encode(texts_for_run, normalize_embeddings=True)
            t1 = time.perf_counter()
            orig_latency = t1 - t0

            with st.spinner("–ó–∞–≥—Ä—É–∑–∫–∞ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏..."):
                quant = QuantModel(
                    model_id=quant_id,
                    source=quant_source,
                    model_dir=quant_dir,
                    tokenizer_name=tokenizer_name if tokenizer_name else None,
                    force_download=force_download
                )
            st.write(f"üîë –ò—Å–ø–æ–ª—å–∑—É–µ–º—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä: `{quant.tokenizer.name_or_path}`")

            t0 = time.perf_counter()
            quant_embs = quant.encode(texts_for_run, normalize=True)
            t1 = time.perf_counter()
            quant_latency = t1 - t0

            O = np.asarray(orig_embs)
            Q = np.asarray(quant_embs)
            if O.ndim == 3:
                O = O.mean(axis=1)
            if Q.ndim == 3:
                Q = Q.mean(axis=1)
            if O.shape[1] != Q.shape[1]:
                m = min(O.shape[1], Q.shape[1])
                O = O[:, :m]
                Q = Q[:, :m]

            per_text_cos = cosine_batch(O, Q)
            avg_cos = float(per_text_cos.mean())
            med_cos = float(np.median(per_text_cos))

            metrics_df = pd.DataFrame([
                {
                    "Mode": "Original",
                    "Batch Size": batch_size,
                    "Latency (s)": orig_latency,
                    "Throughput (texts/s)": len(texts_for_run) / max(orig_latency, 1e-12),
                },
                {
                    "Mode": "Quantized",
                    "Batch Size": batch_size,
                    "Latency (s)": quant_latency,
                    "Throughput (texts/s)": len(texts_for_run) / max(quant_latency, 1e-12),
                },
            ])

            st.subheader("üìä –ú–µ—Ç—Ä–∏–∫–∏ —Å–∫–æ—Ä–æ—Å—Ç–∏")
            st.dataframe(metrics_df)

            st.subheader("üéØ –ö–∞—á–µ—Å—Ç–≤–æ")
            st.write(f"–°—Ä–µ–¥–Ω—è—è cosine similarity (–ø–æ {len(per_text_cos)} —Ç–µ–∫—Å—Ç–∞–º): **{avg_cos:.4f}**")
            st.write(f"–ú–µ–¥–∏–∞–Ω–∞ cosine similarity: **{med_cos:.4f}**")

        if metrics_df is not None:
            csv = metrics_df.to_csv(index=False).encode("utf-8")
            st.download_button(
                label="üì• –°–∫–∞—á–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã (CSV)",
                data=csv,
                file_name="metrics.csv",
                mime="text/csv",
            )

    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞: {e}")
        st.text(traceback.format_exc())
