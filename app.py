import time
import psutil
import numpy as np
import streamlit as st
from transformers import AutoTokenizer, AutoModel
from sentence_transformers import SentenceTransformer
import torch
import onnxruntime as ort
from numpy.linalg import norm
import os
import zipfile
import gdown
import huggingface_hub
from pathlib import Path

# ======================
# –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏
# ======================

def download_model(source, model_id, model_dir):
    """–°–∫–∞—á–∏–≤–∞–µ—Ç –º–æ–¥–µ–ª—å —Å GDrive –∏–ª–∏ HF."""
    model_dir = Path(model_dir)
    model_dir.mkdir(parents=True, exist_ok=True)

    if any(model_dir.glob("*")):
        print(f"üìÇ –ú–æ–¥–µ–ª—å —É–∂–µ –µ—Å—Ç—å –≤ {model_dir}")
        return model_dir

    if source == "gdrive":
        zip_path = f"{model_dir}.zip"
        print(f"üì• –°–∫–∞—á–∏–≤–∞—é –º–æ–¥–µ–ª—å —Å Google Drive: {model_id}")
        gdown.download(f"https://drive.google.com/uc?id={model_id}", str(zip_path), quiet=False)
        with zipfile.ZipFile(zip_path, "r") as zf:
            zf.extractall(model_dir)
        os.remove(zip_path)
    elif source == "hf":
        print(f"üì• –°–∫–∞—á–∏–≤–∞—é –º–æ–¥–µ–ª—å —Å Hugging Face: {model_id}")
        huggingface_hub.snapshot_download(
            repo_id=model_id,
            local_dir=model_dir,
            local_dir_use_symlinks=False
        )
    else:
        raise ValueError(f"‚ùå –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫: {source}")
    return model_dir


def find_quantized_file(model_dir):
    """–ò—â–µ—Ç –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–Ω—ã–π ONNX-—Ñ–∞–π–ª –≤ –ø–∞–ø–∫–µ –º–æ–¥–µ–ª–∏."""
    model_dir = Path(model_dir)
    quant_files = list(model_dir.rglob("model_quantized.onnx"))
    if quant_files:
        print(f"‚úÖ –ù–∞–π–¥–µ–Ω –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–Ω—ã–π —Ñ–∞–π–ª: {quant_files[0]}")
        return str(quant_files[0])
    print("‚ö†Ô∏è –ö–≤–∞–Ω—Ç–æ–≤–∞–Ω–Ω—ã–π —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω, –∑–∞–≥—Ä—É–∂–∞–µ–º –æ–±—ã—á–Ω—É—é –º–æ–¥–µ–ª—å")
    return None


@st.cache_resource
def load_model(model_path, model_type="sentence-transformers", quantized=False):
    """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ (–æ—Ä–∏–≥–∏–Ω–∞–ª –∏–ª–∏ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–Ω–∞—è)."""
    if model_type == "sentence-transformers":
        if quantized:
            quant_file = find_quantized_file(model_path)
            if quant_file:
                return SentenceTransformer(model_path, backend="onnx", model_kwargs={"file_name": Path(quant_file).name})
        return SentenceTransformer(model_path)
    elif model_type == "transformers":
        tokenizer = AutoTokenizer.from_pretrained(model_path)
        model = AutoModel.from_pretrained(model_path)
        return model, tokenizer
    elif model_type == "onnx":
        so = ort.SessionOptions()
        so.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL
        providers = ["CPUExecutionProvider"]
        return ort.InferenceSession(model_path, sess_options=so, providers=providers)


def measure_resources(func, *args, **kwargs):
    """–ò–∑–º–µ—Ä–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏, RAM –∏ CPU."""
    process = psutil.Process()
    start_mem = process.memory_info().rss / 1024**2
    start_cpu = psutil.cpu_percent(interval=None)

    start_time = time.time()
    result = func(*args, **kwargs)
    end_time = time.time()

    end_mem = process.memory_info().rss / 1024**2
    end_cpu = psutil.cpu_percent(interval=None)

    return {
        "result": result,
        "time": end_time - start_time,
        "ram_used": end_mem - start_mem,
        "cpu": end_cpu
    }


def cosine_similarity(vec1, vec2):
    """–ö–æ—Å–∏–Ω—É—Å–Ω–∞—è —Å—Ö–æ–∂–µ—Å—Ç—å."""
    return np.dot(vec1, vec2) / (norm(vec1) * norm(vec2))


# ======================
# –ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å Streamlit
# ======================

st.title("üîç –¢–µ—Å—Ç–µ—Ä –∫–≤–∞–Ω—Ç–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π")

# –í–≤–æ–¥ —Ç–µ–∫—Å—Ç–∞
input_text = st.text_area("–í–≤–µ–¥–∏—Ç–µ —Ç–µ–∫—Å—Ç:", "–≠—Ç–æ —Ç–µ—Å—Ç–æ–≤–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ.")

# –í—ã–±–æ—Ä –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –º–æ–¥–µ–ª–∏
source_choice = st.radio("–í—ã–±–µ—Ä–∏—Ç–µ –∏—Å—Ç–æ—á–Ω–∏–∫ –º–æ–¥–µ–ª–∏:", ["gdrive", "hf"])

# ID –º–æ–¥–µ–ª–µ–π
original_id = st.text_input("ID/Repo –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏:", "deepvk/USER-BGE-M3")
quantized_id = st.text_input("ID/Repo –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏:", "1lkrvCPIE1wvffIuCSHGtbEz3Epjx5R36")

# –ö–Ω–æ–ø–∫–∞ –∑–∞–ø—É—Å–∫–∞
if st.button("üîé –ó–∞–ø—É—Å—Ç–∏—Ç—å —Ç–µ—Å—Ç"):
    st.write("‚è≥ –°–∫–∞—á–∏–≤–∞—é –∏ –∑–∞–≥—Ä—É–∂–∞—é –º–æ–¥–µ–ª–∏...")

    # –°–∫–∞—á–∏–≤–∞–µ–º –º–æ–¥–µ–ª–∏
    orig_dir = download_model(source_choice, original_id, "original_model")
    quant_dir = download_model(source_choice, quantized_id, "quantized_model")

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª–∏
    original_model = load_model(str(orig_dir), model_type="sentence-transformers")
    quantized_model = load_model(str(quant_dir), model_type="sentence-transformers", quantized=True)

    # –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ
    st.write("‚ö° –ò–∑–º–µ—Ä—è—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏...")
    orig = measure_resources(original_model.encode, [input_text], normalize_embeddings=True)

    st.write("‚ö° –ò–∑–º–µ—Ä—è—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏...")
    quant = measure_resources(quantized_model.encode, [input_text], normalize_embeddings=True)

    # –ö–∞—á–µ—Å—Ç–≤–æ
    similarity = cosine_similarity(orig["result"][0], quant["result"][0])

    # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    st.subheader("üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã")
    st.write(f"**–í—Ä–µ–º—è (–æ—Ä–∏–≥–∏–Ω–∞–ª):** {orig['time']:.4f} —Å–µ–∫")
    st.write(f"**–í—Ä–µ–º—è (–∫–≤–∞–Ω—Ç):** {quant['time']:.4f} —Å–µ–∫")
    st.write(f"**RAM (–æ—Ä–∏–≥–∏–Ω–∞–ª):** {orig['ram_used']:.2f} MB")
    st.write(f"**RAM (–∫–≤–∞–Ω—Ç):** {quant['ram_used']:.2f} MB")
    st.write(f"**CPU –Ω–∞–≥—Ä—É–∑–∫–∞:** {quant['cpu']}%")
    st.write(f"**–ö–æ—Å–∏–Ω—É—Å–Ω–∞—è —Å—Ö–æ–∂–µ—Å—Ç—å:** {similarity:.4f}")

    # –ì—Ä–∞—Ñ–∏–∫–∏
    st.bar_chart({
        "–í—Ä–µ–º—è (—Å–µ–∫)": [orig["time"], quant["time"]],
        "RAM (MB)": [orig["ram_used"], quant["ram_used"]]
    })
