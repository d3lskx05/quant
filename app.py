import os
import zipfile
import time
import traceback
from pathlib import Path
from functools import lru_cache
from typing import Optional

import gdown
import huggingface_hub
import numpy as np
import psutil
import pandas as pd
import streamlit as st
import onnxruntime as ort
from numpy.linalg import norm
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer

st.set_page_config(page_title="Quantized model tester", layout="wide")

# ============================================================
# üî• QuantModel
# ============================================================
class QuantModel:
    """–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –∑–∞–≥—Ä—É–∑—á–∏–∫ –∫–≤–∞–Ω—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö ONNX –º–æ–¥–µ–ª–µ–π."""
    def __init__(self, model_id: str, source: str = "gdrive",
                 model_dir: str = "onnx_model", tokenizer_name: Optional[str] = None,
                 force_download: bool = False):
        self.model_id = model_id
        self.source = source
        self.model_dir = Path(model_dir)
        self.tokenizer_name = tokenizer_name
        self.force_download = force_download
        self.model_path = None

        self._ensure_model()
        self.session = self._load_session()
        self.tokenizer = self._load_tokenizer()

    def _ensure_model(self):
        os.makedirs(self.model_dir, exist_ok=True)
        need_download = self.force_download or not any(self.model_dir.glob("*.onnx"))

        if need_download:
            if self.source == "gdrive":
                zip_path = f"{self.model_dir}.zip"
                gdown.download(f"https://drive.google.com/uc?id={self.model_id}", zip_path, quiet=False)
                with zipfile.ZipFile(zip_path, "r") as zf:
                    zf.extractall(self.model_dir)
                os.remove(zip_path)
            elif self.source == "hf":
                huggingface_hub.snapshot_download(
                    repo_id=self.model_id,
                    local_dir=self.model_dir,
                    local_dir_use_symlinks=False,
                    resume_download=True
                )

        onnx_files = list(self.model_dir.rglob("*.onnx"))
        if not onnx_files:
            raise FileNotFoundError(f"‚ùå –ù–µ—Ç .onnx –º–æ–¥–µ–ª–∏ –≤ {self.model_dir}")
        self.model_path = onnx_files[0]

    def _load_session(self):
        so = ort.SessionOptions()
        so.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL
        providers = ["CPUExecutionProvider"]
        try:
            if ort.get_device() == "GPU":
                providers.insert(0, "CUDAExecutionProvider")
        except Exception:
            pass
        return ort.InferenceSession(str(self.model_path), sess_options=so, providers=providers)

    def _load_tokenizer(self):
        if self.tokenizer_name:
            return AutoTokenizer.from_pretrained(self.tokenizer_name)
        try:
            return AutoTokenizer.from_pretrained(str(self.model_dir))
        except Exception:
            return AutoTokenizer.from_pretrained("deepvk/USER-BGE-M3")

    @lru_cache(maxsize=1024)
    def _encode_cached(self, text: str, normalize: bool = True):
        inputs = self.tokenizer([text], padding=True, truncation=True, return_tensors="np")
        ort_inputs = {k: v for k, v in inputs.items()}
        embeddings = self.session.run(None, ort_inputs)[0]
        if normalize:
            norms = np.linalg.norm(embeddings, axis=1, keepdims=True) + 1e-10
            embeddings = embeddings / norms
        return embeddings[0]

    def encode(self, texts, normalize=True):
        if isinstance(texts, str):
            texts = [texts]
        return np.array([self._encode_cached(t, normalize) for t in texts])


# ============================================================
# üîß Helpers
# ============================================================
def to_vector(embs):
    arr = np.array(embs)
    if arr.ndim == 1:
        return arr
    if arr.ndim == 2 and arr.shape[0] == 1:
        return arr[0]
    return arr.mean(axis=0)


def cosine_similarity(vec1, vec2):
    return float(np.dot(vec1, vec2) / ((norm(vec1) * norm(vec2)) + 1e-12))


# ============================================================
# üéõÔ∏è UI
# ============================================================
st.title("üîç –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–µ–π: –û—Ä–∏–≥–∏–Ω–∞–ª vs –ö–≤–∞–Ω—Ç")

mode = st.radio("–í—ã–±–µ—Ä–∏—Ç–µ —Ä–µ–∂–∏–º:", ["–û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å", "–ö–≤–∞–Ω—Ç–æ–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å"])

input_text = st.text_area("–¢–µ–∫—Å—Ç—ã –¥–ª—è —Ç–µ—Å—Ç–∞ (–ø–æ –æ–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–µ)", "–≠—Ç–æ —Ç–µ—Å—Ç–æ–≤–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ.\n–ü—Ä–∏–º–µ—Ä –≤—Ç–æ—Ä–æ–π —Å—Ç—Ä–æ–∫–∏.")
texts = [t.strip() for t in input_text.split("\n") if t.strip()]

batch_size = st.slider("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–≤—Ç–æ—Ä–æ–≤ –¥–ª—è throughput-—Ç–µ—Å—Ç–∞", 1, 128, 8)
force_download = st.checkbox("‚ôªÔ∏è –ü–µ—Ä–µ–∫–∞—á–∞—Ç—å –º–æ–¥–µ–ª—å –∑–∞–Ω–æ–≤–æ", False)

metrics = {}

if mode == "–û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å":
    model_id = st.text_input("HF repo ID", "deepvk/USER-BGE-M3")
else:
    col1, col2 = st.columns(2)
    with col1:
        quant_source = st.selectbox("–ò—Å—Ç–æ—á–Ω–∏–∫", ["gdrive", "hf", "local"], index=1)
        quant_id = st.text_input("ID/Repo/Path", "1lkrvCPIE1wvffIuCSHGtbEz3Epjx5R36")
    with col2:
        quant_dir = st.text_input("–ü–∞–ø–∫–∞ –¥–ª—è –∫–≤–∞–Ω—Ç–∞", "onnx-user-bge-m3")
        tokenizer_name = st.text_input("Tokenizer name", "")

run_button = st.button("üöÄ –ó–∞–ø—É—Å—Ç–∏—Ç—å —Ç–µ—Å—Ç")

# ============================================================
# üöÄ –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–∞
# ============================================================
if run_button:
    try:
        proc = psutil.Process()
        texts_for_run = (texts * batch_size)[:max(len(texts), 1)]

        if mode == "–û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å":
            with st.spinner("–ó–∞–≥—Ä—É–∑–∫–∞ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏..."):
                model = SentenceTransformer(model_id)
            t0 = time.perf_counter()
            embs = model.encode(texts_for_run, normalize_embeddings=True)
            t1 = time.perf_counter()
        else:
            with st.spinner("–ó–∞–≥—Ä—É–∑–∫–∞ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏..."):
                model = QuantModel(
                    model_id=quant_id,
                    source=quant_source,
                    model_dir=quant_dir,
                    tokenizer_name=tokenizer_name if tokenizer_name else None,
                    force_download=force_download
                )
            t0 = time.perf_counter()
            embs = model.encode(texts_for_run, normalize=True)
            t1 = time.perf_counter()

        latency = t1 - t0
        memory = proc.memory_info().rss / 1024 ** 2

        metrics = {
            "Mode": [mode],
            "Batch Size": [batch_size],
            "Latency (s)": [latency],
            "Memory (MB)": [memory],
        }

        st.subheader("üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã")
        st.metric("Latency (s)", f"{latency:.4f}")
        st.metric("Memory (MB)", f"{memory:.1f}")

        df = pd.DataFrame(metrics)
        st.dataframe(df)

        csv = df.to_csv(index=False).encode("utf-8")
        st.download_button(
            label="üì• –°–∫–∞—á–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã (CSV)",
            data=csv,
            file_name="metrics.csv",
            mime="text/csv",
        )

    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞: {e}")
        st.text(traceback.format_exc())
