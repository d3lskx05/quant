import time
import psutil
import numpy as np
import streamlit as st
from sentence_transformers import SentenceTransformer
import onnxruntime as ort
from numpy.linalg import norm
import os
import zipfile
import gdown
import huggingface_hub
from pathlib import Path


# ======================
# –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏
# ======================

@st.cache_resource
def download_model(source, model_id, model_dir):
    """–°–∫–∞—á–∏–≤–∞–µ—Ç –º–æ–¥–µ–ª—å —Å GDrive –∏–ª–∏ HF (—Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º)."""
    model_dir = Path(model_dir)
    model_dir.mkdir(parents=True, exist_ok=True)

    if any(model_dir.glob("*")):
        return str(model_dir)

    if source == "gdrive":
        zip_path = f"{model_dir}.zip"
        gdown.download(f"https://drive.google.com/uc?id={model_id}", str(zip_path), quiet=False)
        with zipfile.ZipFile(zip_path, "r") as zf:
            zf.extractall(model_dir)
        os.remove(zip_path)
    elif source == "hf":
        huggingface_hub.snapshot_download(
            repo_id=model_id,
            local_dir=model_dir,
            local_dir_use_symlinks=False
        )
    else:
        raise ValueError(f"‚ùå –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫: {source}")
    return str(model_dir)


def find_quantized_file(model_dir):
    """–ò—â–µ—Ç –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–Ω—ã–π ONNX-—Ñ–∞–π–ª –≤ –ø–∞–ø–∫–µ –º–æ–¥–µ–ª–∏."""
    model_dir = Path(model_dir)
    quant_files = list(model_dir.rglob("model_quantized.onnx"))
    return str(quant_files[0]) if quant_files else None


@st.cache_resource
def load_model(model_path, quantized=False):
    """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏: SentenceTransformers –∏–ª–∏ ONNX –Ω–∞–ø—Ä—è–º—É—é."""
    if quantized:
        quant_file = find_quantized_file(model_path)
        if quant_file:
            so = ort.SessionOptions()
            so.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL
            return ort.InferenceSession(quant_file, sess_options=so, providers=["CPUExecutionProvider"])
        else:
            st.warning("‚ö†Ô∏è –ö–≤–∞–Ω—Ç–æ–≤–∞–Ω–Ω—ã–π —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω, –∑–∞–≥—Ä—É–∂–∞–µ–º –æ–±—ã—á–Ω—É—é –º–æ–¥–µ–ª—å")
    return SentenceTransformer(model_path)


def encode_onnx(session, tokenizer, text):
    """–ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ —á–µ—Ä–µ–∑ —á–∏—Å—Ç—ã–π ONNX Runtime."""
    import torch
    from transformers import AutoTokenizer

    inputs = tokenizer(text, return_tensors="np", padding=True, truncation=True)
    ort_inputs = {k: v for k, v in inputs.items()}
    ort_outs = session.run(None, ort_inputs)
    return ort_outs[0]


def measure_resources(func, *args, **kwargs):
    """–ò–∑–º–µ—Ä–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏, RAM –∏ CPU."""
    process = psutil.Process()
    start_mem = process.memory_info().rss / 1024**2
    start_cpu = psutil.cpu_percent(interval=None)

    start_time = time.time()
    result = func(*args, **kwargs)
    end_time = time.time()

    end_mem = process.memory_info().rss / 1024**2
    end_cpu = psutil.cpu_percent(interval=None)

    return {
        "result": result,
        "time": end_time - start_time,
        "ram_used": end_mem - start_mem,
        "cpu": end_cpu
    }


def cosine_similarity(vec1, vec2):
    """–ö–æ—Å–∏–Ω—É—Å–Ω–∞—è —Å—Ö–æ–∂–µ—Å—Ç—å."""
    return np.dot(vec1, vec2) / (norm(vec1) * norm(vec2))


# ======================
# –ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å Streamlit
# ======================

st.title("üîç –¢–µ—Å—Ç–µ—Ä –∫–≤–∞–Ω—Ç–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π")

# –í–≤–æ–¥ —Ç–µ–∫—Å—Ç–∞
input_text = st.text_area("–í–≤–µ–¥–∏—Ç–µ —Ç–µ–∫—Å—Ç:", "–≠—Ç–æ —Ç–µ—Å—Ç–æ–≤–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ.")

# –í—ã–±–æ—Ä –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
orig_source = st.radio("–ò—Å—Ç–æ—á–Ω–∏–∫ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏:", ["gdrive", "hf"], index=1, key="orig")
quant_source = st.radio("–ò—Å—Ç–æ—á–Ω–∏–∫ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏:", ["gdrive", "hf"], index=0, key="quant")

# ID –º–æ–¥–µ–ª–µ–π
original_id = st.text_input("ID/Repo –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏:", "deepvk/USER-BGE-M3")
quantized_id = st.text_input("ID/Repo –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏:", "1lkrvCPIE1wvffIuCSHGtbEz3Epjx5R36")

# –ö–Ω–æ–ø–∫–∞ –∑–∞–ø—É—Å–∫–∞
if st.button("üîé –ó–∞–ø—É—Å—Ç–∏—Ç—å —Ç–µ—Å—Ç"):
    st.write("‚è≥ –°–∫–∞—á–∏–≤–∞—é –∏ –∑–∞–≥—Ä—É–∂–∞—é –º–æ–¥–µ–ª–∏...")

    orig_dir = download_model(orig_source, original_id, "original_model")
    quant_dir = download_model(quant_source, quantized_id, "quantized_model")

    original_model = load_model(orig_dir, quantized=False)
    quantized_model = load_model(quant_dir, quantized=True)

    st.write("‚ö° –ò–∑–º–µ—Ä—è—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏...")
    orig = measure_resources(original_model.encode, [input_text], normalize_embeddings=True)

    st.write("‚ö° –ò–∑–º–µ—Ä—è—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏...")
    if isinstance(quantized_model, ort.InferenceSession):
        from transformers import AutoTokenizer
        tokenizer = AutoTokenizer.from_pretrained(orig_dir)
        quant = measure_resources(encode_onnx, quantized_model, tokenizer, [input_text])
    else:
        quant = measure_resources(quantized_model.encode, [input_text], normalize_embeddings=True)

    similarity = cosine_similarity(orig["result"][0], quant["result"][0])

    st.subheader("üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã")
    st.write(f"**–í—Ä–µ–º—è (–æ—Ä–∏–≥–∏–Ω–∞–ª):** {orig['time']:.4f} —Å–µ–∫")
    st.write(f"**–í—Ä–µ–º—è (–∫–≤–∞–Ω—Ç):** {quant['time']:.4f} —Å–µ–∫")
    st.write(f"**RAM (–æ—Ä–∏–≥–∏–Ω–∞–ª):** {orig['ram_used']:.2f} MB")
    st.write(f"**RAM (–∫–≤–∞–Ω—Ç):** {quant['ram_used']:.2f} MB")
    st.write(f"**CPU –Ω–∞–≥—Ä—É–∑–∫–∞:** {quant['cpu']}%")
    st.write(f"**–ö–æ—Å–∏–Ω—É—Å–Ω–∞—è —Å—Ö–æ–∂–µ—Å—Ç—å:** {similarity:.4f}")

    st.bar_chart({
        "–í—Ä–µ–º—è (—Å–µ–∫)": [orig["time"], quant["time"]],
        "RAM (MB)": [orig["ram_used"], quant["ram_used"]]
    })
