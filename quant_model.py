# quant_model.py
import os
import zipfile
import gdown
import numpy as np
import onnxruntime as ort
from pathlib import Path
from functools import lru_cache
from transformers import AutoTokenizer
import huggingface_hub


class QuantModel:
    """
    –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –∑–∞–≥—Ä—É–∑—á–∏–∫ –∫–≤–∞–Ω—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö ONNX –º–æ–¥–µ–ª–µ–π.
    - –ò—Å—Ç–æ—á–Ω–∏–∫–∏: Google Drive (gdrive), Hugging Face Hub (hf), –ª–æ–∫–∞–ª—å–Ω–∞—è (local)
    - –ê–≤—Ç–æ–ø–æ–∏—Å–∫ .onnx —Ñ–∞–π–ª–∞
    - –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
    """

    def __init__(self, model_id: str, source: str = "gdrive",
                 model_dir: str = "onnx_model", tokenizer_name: str = None):
        self.model_id = model_id
        self.source = source
        self.model_dir = Path(model_dir)
        self.tokenizer_name = tokenizer_name
        self.model_path = None

        self._ensure_model()
        self.session = self._load_session()
        self.tokenizer = self._load_tokenizer()

    # ========================
    # üì• –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
    # ========================
    def _ensure_model(self):
        """–°–∫–∞—á–∏–≤–∞–Ω–∏–µ –∏ —Ä–∞—Å–ø–∞–∫–æ–≤–∫–∞ –º–æ–¥–µ–ª–∏, –µ—Å–ª–∏ –æ–Ω–∞ –Ω–µ –ª–æ–∫–∞–ª—å–Ω–∞—è."""
        os.makedirs(self.model_dir, exist_ok=True)

        if not any(self.model_dir.glob("*.onnx")):
            if self.source == "gdrive":
                zip_path = f"{self.model_dir}.zip"
                print(f"üì• –°–∫–∞—á–∏–≤–∞—é –º–æ–¥–µ–ª—å —Å Google Drive: {self.model_id}")
                gdown.download(f"https://drive.google.com/uc?id={self.model_id}", zip_path, quiet=False)
                print(f"üì¶ –†–∞—Å–ø–∞–∫–æ–≤–∫–∞ {zip_path}...")
                with zipfile.ZipFile(zip_path, "r") as zf:
                    zf.extractall(self.model_dir)
                os.remove(zip_path)

            elif self.source == "hf":
                print(f"üì• –°–∫–∞—á–∏–≤–∞—é –º–æ–¥–µ–ª—å —Å Hugging Face: {self.model_id}")
                huggingface_hub.snapshot_download(
                    repo_id=self.model_id,
                    local_dir=self.model_dir,
                    local_dir_use_symlinks=False
                )

            elif self.source == "local":
                print(f"üìÇ –ò—Å–ø–æ–ª—å–∑—É—é –ª–æ–∫–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å: {self.model_dir}")
            else:
                raise ValueError(f"‚ùå –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫: {self.source}")

        # –ò—â–µ–º –ø–µ—Ä–≤—ã–π .onnx
        onnx_files = list(self.model_dir.rglob("*.onnx"))
        if not onnx_files:
            raise FileNotFoundError(f"‚ùå –í {self.model_dir} –Ω–µ –Ω–∞–π–¥–µ–Ω .onnx —Ñ–∞–π–ª!")
        self.model_path = onnx_files[0]
        print(f"‚úÖ –ù–∞–π–¥–µ–Ω ONNX —Ñ–∞–π–ª: {self.model_path}")

    # ========================
    # üöÄ –ó–∞–≥—Ä—É–∑–∫–∞ ONNX Session
    # ========================
    def _load_session(self):
        so = ort.SessionOptions()
        so.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL
        providers = ["CPUExecutionProvider"]
        try:
            if ort.get_device() == "GPU":
                providers.insert(0, "CUDAExecutionProvider")
        except Exception:
            pass
        print(f"üöÄ –ó–∞–≥—Ä—É–∂–∞—é –º–æ–¥–µ–ª—å –Ω–∞ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞—Ö: {providers}")
        return ort.InferenceSession(str(self.model_path), sess_options=so, providers=providers)

    # ========================
    # üìù –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä–∞
    # ========================
    def _load_tokenizer(self):
        if self.tokenizer_name:
            return AutoTokenizer.from_pretrained(self.tokenizer_name)
        try:
            return AutoTokenizer.from_pretrained(str(self.model_dir))
        except Exception:
            print("‚ö†Ô∏è –¢–æ–∫–µ–Ω–∞–π–∑–µ—Ä –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –ø–∞–ø–∫–µ, –∏—Å–ø–æ–ª—å–∑—É–µ–º deepvk/USER-BGE-M3")
            return AutoTokenizer.from_pretrained("deepvk/USER-BGE-M3")

    # ========================
    # üî• –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ
    # ========================
    @lru_cache(maxsize=1024)
    def _encode_cached(self, text: str, normalize: bool = True):
        inputs = self.tokenizer([text], padding=True, truncation=True, return_tensors="np")
        ort_inputs = {k: v for k, v in inputs.items()}
        embeddings = self.session.run(None, ort_inputs)[0]

        if normalize:
            norms = np.linalg.norm(embeddings, axis=1, keepdims=True) + 1e-10
            embeddings = embeddings / norms
        return embeddings[0]

    def encode(self, texts, normalize=True):
        if isinstance(texts, str):
            texts = [texts]
        return np.array([self._encode_cached(t, normalize) for t in texts])


# ========================
# üîó –ì–ª–æ–±–∞–ª—å–Ω—ã–π –¥–æ—Å—Ç—É–ø
# ========================
@lru_cache(maxsize=1)
def get_model():
    model_id = os.getenv("MODEL_ID", "1lkrvCPIE1wvffIuCSHGtbEz3Epjx5R36")
    source = os.getenv("MODEL_SOURCE", "gdrive")
    model_dir = os.getenv("MODEL_DIR", "onnx-user-bge-m3")
    tokenizer = os.getenv("TOKENIZER_NAME", None)
    return QuantModel(model_id, source, model_dir, tokenizer)
